# 按照 dt_adcode 、date、flag 排序
combined_data <- combined_data %>%
arrange(dt_adcode, date,flag)
View(combined_data)
# 按照 dt_adcode 、date、flag 排序
combined_data <- combined_data %>%
arrange(dt_adcode, date,flag)
# 追加一个变量group，连续两行为一组，group=1，2，3，...
combined_data <- combined_data %>%
mutate(group = (row_number() + 1) %/% 2)
# read D:\xwechat_files\wxid_vu6eu977ndi121_3458\msg\file\2025-09\matched_data.csv
matched_data <- read.csv("D:\\xwechat_files\\wxid_vu6eu977ndi121_3458\\msg\\file\\2025-09\\matched_data.csv", stringsAsFactors = FALSE)
head(matched_data)
# 计算第一天的停电时间，首先比较date_开始和date_end 是不是同一天，如果是 计算开始时间到结束时间的差值；如果不是计算开始时间到当天24点的差值
library(lubridate)
library(dplyr)
matched_data <- matched_data %>%
mutate(
date_开始 = as.Date(date_开始),
date_结束 = as.Date(date_结束),
开始时间 = hms::as_hms(parse_date_time(开始时间, orders = "HM")),
结束时间 = hms::as_hms(parse_date_time(结束时间, orders = "HM")),
停电时长_第一天 = ifelse(date_开始 == date_结束,
as.numeric(difftime(结束时间, 开始时间, units = "hours")),
as.numeric(difftime(hms::as_hms("24:00:00"), 开始时间, units = "hours")))
)
#
# 计算date_结束的停电时间，如果date_结束与date_开始不是同一天，则计算0点到结束时间的差值；如果是同一天则为停电时长_第一天的值
matched_data <- matched_data %>%
mutate(
停电时长_最后一天 = ifelse(date_开始 != date_结束,
as.numeric(difftime(结束时间, hms::as_hms("00:00:00"), units = "hours")),
停电时长_第一天)
)
# 加一个变量为group，值为行数
matched_data <- matched_data %>%
mutate(group = row_number())
# 提取 dt_adcode date_开始 停电时长_第一天， （date_开始 停电时长_第一天）命名为 （date, times），追加新变量flag，值为1; 提取 dt_adcode date_结束 停电时长_最后一天， （date_结束 停电时长_最后一天）命名为 （date, times）追加新变量flag值为2,然后两个数据库纵向合并
start_data <- matched_data %>%
select(dt_adcode, date_开始, 停电时长_第一天，group) %>%
# read D:\xwechat_files\wxid_vu6eu977ndi121_3458\msg\file\2025-09\matched_data.csv
matched_data <- read.csv("D:\\xwechat_files\\wxid_vu6eu977ndi121_3458\\msg\\file\\2025-09\\matched_data.csv", stringsAsFactors = FALSE)
head(matched_data)
# 计算第一天的停电时间，首先比较date_开始和date_end 是不是同一天，如果是 计算开始时间到结束时间的差值；如果不是计算开始时间到当天24点的差值
library(lubridate)
library(dplyr)
matched_data <- matched_data %>%
mutate(
date_开始 = as.Date(date_开始),
date_结束 = as.Date(date_结束),
开始时间 = hms::as_hms(parse_date_time(开始时间, orders = "HM")),
结束时间 = hms::as_hms(parse_date_time(结束时间, orders = "HM")),
停电时长_第一天 = ifelse(date_开始 == date_结束,
as.numeric(difftime(结束时间, 开始时间, units = "hours")),
as.numeric(difftime(hms::as_hms("24:00:00"), 开始时间, units = "hours")))
)
#
# 计算date_结束的停电时间，如果date_结束与date_开始不是同一天，则计算0点到结束时间的差值；如果是同一天则为停电时长_第一天的值
matched_data <- matched_data %>%
mutate(
停电时长_最后一天 = ifelse(date_开始 != date_结束,
as.numeric(difftime(结束时间, hms::as_hms("00:00:00"), units = "hours")),
停电时长_第一天)
)
# 加一个变量为group，值为行数
matched_data <- matched_data %>%
mutate(group = row_number())
# 提取 dt_adcode date_开始 停电时长_第一天， （date_开始 停电时长_第一天）命名为 （date, times），追加新变量flag，值为1; 提取 dt_adcode date_结束 停电时长_最后一天， （date_结束 停电时长_最后一天）命名为 （date, times）追加新变量flag值为2,然后两个数据库纵向合并
start_data <- matched_data %>%
select(dt_adcode, date_开始, 停电时长_第一天,group) %>%
rename(date = date_开始, times = 停电时长_第一天) %>%
mutate(flag = 1)
end_data <- matched_data %>%
select(dt_adcode, date_结束, 停电时长_最后一天,group) %>%
rename(date = date_结束, times = 停电时长_最后一天) %>%
mutate(flag = 2)
combined_data <- rbind(start_data, end_data)
head(combined_data)
# 按照 dt_adcode 、date、flag 排序
combined_data <- combined_data %>%
arrange(group, date,flag)
head(combined_data)
# 在每个group里面，计算flag=1到2之间的天数
combined_data <- combined_data %>%
group_by(group) %>%
mutate(days_diff = max(as.Date(date))-min(as.Date(date))) %>%
ungroup()
head(combined_data)
# 打印出days_diff的统计结果
print(summary(combined_data$days_diff))
library(tidyr)
combined_data <- combined_data %>%
group_by(dt_adcode, group) %>%
complete(date = seq(min(date), max(date), by = "day")) %>%
ungroup()
head(combined_data)
# 打印出days_diff 大于5的数据
long_gaps <- combined_data %>%
filter(days_diff > 5)
print(long_gaps)
print(n = ...)
# 打印出days_diff 大于5的数据
long_gaps <- combined_data %>%
filter(days_diff > 5)
print(long_gaps)
# read D:\xwechat_files\wxid_vu6eu977ndi121_3458\msg\file\2025-09\matched_data.csv
matched_data <- read.csv("D:\\xwechat_files\\wxid_vu6eu977ndi121_3458\\msg\\file\\2025-09\\matched_data.csv", stringsAsFactors = FALSE)
head(matched_data)
# 计算第一天的停电时间，首先比较date_开始和date_end 是不是同一天，如果是 计算开始时间到结束时间的差值；如果不是计算开始时间到当天24点的差值
library(lubridate)
library(dplyr)
matched_data <- matched_data %>%
mutate(
date_开始 = as.Date(date_开始),
date_结束 = as.Date(date_结束),
开始时间 = hms::as_hms(parse_date_time(开始时间, orders = "HM")),
结束时间 = hms::as_hms(parse_date_time(结束时间, orders = "HM")),
停电时长_第一天 = ifelse(date_开始 == date_结束,
as.numeric(difftime(结束时间, 开始时间, units = "hours")),
as.numeric(difftime(hms::as_hms("24:00:00"), 开始时间, units = "hours")))
)
#
# 计算date_结束的停电时间，如果date_结束与date_开始不是同一天，则计算0点到结束时间的差值；如果是同一天则为停电时长_第一天的值
matched_data <- matched_data %>%
mutate(
停电时长_最后一天 = ifelse(date_开始 != date_结束,
as.numeric(difftime(结束时间, hms::as_hms("00:00:00"), units = "hours")),
停电时长_第一天)
)
# 加一个变量为group，值为行数
matched_data <- matched_data %>%
mutate(group = row_number())
# 提取 dt_adcode date_开始 停电时长_第一天， （date_开始 停电时长_第一天）命名为 （date, times），追加新变量flag，值为1; 提取 dt_adcode date_结束 停电时长_最后一天， （date_结束 停电时长_最后一天）命名为 （date, times）追加新变量flag值为2,然后两个数据库纵向合并
start_data <- matched_data %>%
select(dt_adcode, date_开始, 停电时长_第一天,group) %>%
rename(date = date_开始, times = 停电时长_第一天) %>%
mutate(flag = 1)
end_data <- matched_data %>%
select(dt_adcode, date_结束, 停电时长_最后一天,group) %>%
rename(date = date_结束, times = 停电时长_最后一天) %>%
mutate(flag = 2)
combined_data <- rbind(start_data, end_data)
head(combined_data)
# 按照 dt_adcode 、date、flag 排序
combined_data <- combined_data %>%
arrange(group, date,flag)
head(combined_data)
# 在每个group里面，计算flag=1到2之间的天数
combined_data <- combined_data %>%
group_by(group) %>%
mutate(days_diff = max(as.Date(date))-min(as.Date(date))) %>%
ungroup()
head(combined_data)
# 打印出days_diff的统计结果
print(summary(combined_data$days_diff))
library(tidyr)
combined_data <- combined_data %>%
group_by(dt_adcode, group) %>%
complete(date = seq(min(as.Date(date)), max(as.Date(date)), by = "day")) %>%
ungroup()
head(combined_data)
# 除了times列，其他列都向下填充
combined_data <- combined_data %>%
arrange(group, date,flag)
combined_data <- combined_data %>%
group_by(dt_adcode, group) %>%
fill(c(dt_adcode, group, days_diff), .direction = "down") %>%
ungroup()
head(combined_data)
# 打印出days_diff 大于5的数据
long_gaps <- combined_data %>%
filter(days_diff > 5)
print(long_gaps)
# 将times缺失值改为24
combined_data <- combined_data %>%
mutate(times = ifelse(is.na(times), 24, times))
#删除每个group的date重复值
combined_data <- combined_data %>%
distinct(dt_adcode, date, .keep_all = TRUE)
head(combined_data)
# 将combined_data 按照 dt_adcode, date 排序
combined_data <- combined_data %>%
arrange(dt_adcode, date)
head(combined_data)
# 保留dt_adcode, date, times, days_diff 列,将times命名为hours
combined_data <- combined_data %>%
select(dt_adcode, date, times, days_diff) %>%
rename(hours = times)
head(combined_data)
# 将combined_data 保存为csv文件
write.csv(combined_data, "D:\\xwechat_files\\wxid_vu6eu977ndi121_3458\\msg\\file\\2025-09\\combined_data.csv", row.names = FALSE)
matched_data <- read.csv("D:\\xwechat_files\\wxid_vu6eu977ndi121_3458\\msg\\file\\2025-09\\matched_data.csv", stringsAsFactors = FALSE)
head(matched_data)
# 查看各列的数据类型
str(matched_data)
library(dplyr)
gen_rownum <- function(.data, varname, ...) {
.data %>% group_by(...) %>%
mutate(!!sym(varname) := row_number()) %>%
ungroup()
}
# Example data frame
df <- data.frame(group = c("A", "A", "B", "B", "B"), value = c(10, 20, 30, 40, 50))
# Add a row number within each group
df_with_rownum <- gen_rownum(df, "rownum", group)
print(df_with_rownum)
# Add a row number within each group
name <- "rownum2"
df_with_rownum <- gen_rownum(df, name, group)
print(df_with_rownum)
sopen <- function(..., verbose = TRUE) {
args <- list(...)
if (length(args) == 0) {
target <- getwd()
if (verbose) message("Opening: ", target)
return(invisible(.sopen_open(target)))
}
raw <- paste(unlist(args), collapse = " ")
cleaned <- .sopen_strip_quotes(raw)
if (.sopen_is_url(cleaned)) {
if (verbose) message("Opening URL: ", cleaned)
utils::browseURL(cleaned)
return(invisible(cleaned))
}
cleaned <- .sopen_normalize_slashes(cleaned)
candidate <- .sopen_resolve_candidate(cleaned)
open_target <- if (.sopen_exists(candidate)) candidate else cleaned
if (verbose) message("Opening: ", open_target)
invisible(.sopen_open(open_target))
}
.sopen_strip_quotes <- function(x) {
x <- trimws(x)
if (nchar(x) >= 2 && substr(x, 1, 1) %in% c('"', "'") &&
substr(x, nchar(x), nchar(x)) == substr(x, 1, 1)) {
substr(x, 2, nchar(x) - 1)
} else {
x
}
}
.sopen_is_url <- function(x) {
grepl("^(https?://)", x, ignore.case = TRUE)
}
.sopen_exists <- function(path) {
file.exists(path) || dir.exists(path)
}
.sopen_normalize_slashes <- function(path) {
if (.Platform$OS.type == "windows") {
chartr("/", "\\\\", path)
} else {
chartr("\\\\", "/", path)
}
}
.sopen_resolve_candidate <- function(path) {
expanded <- path.expand(path)
if (.sopen_exists(expanded)) {
normalizePath(expanded, winslash = if (.Platform$OS.type == "windows") "\\" else "/", mustWork = FALSE)
} else if (.Platform$OS.type != "windows" &&
grepl("\\.app$", expanded, ignore.case = TRUE)) {
app_path <- file.path("/Applications", basename(expanded))
if (dir.exists(app_path)) return(app_path)
expanded
} else {
expanded
}
}
.sopen_open <- function(target) {
if (.Platform$OS.type == "windows") {
target <- normalizePath(target, winslash = "\\", mustWork = FALSE)
utils::shell.exec(target)
} else {
opener <- if (Sys.info()[["sysname"]] == "Darwin") "open" else "xdg-open"
status <- suppressWarnings(system2(opener, shQuote(target), wait = FALSE, stdout = NULL, stderr = NULL))
if (!identical(status, 0L)) stop(sprintf("Failed to open '%s' using %s.", target, opener), call. = FALSE)
}
target
}
sopen("D:/")
sopen("D:\")
)
}
)))))
//
\\\
""
"
sopen("D:/")
sopen <- function(..., verbose = TRUE) {
args <- list(...)
if (length(args) == 0) {
target <- getwd()
if (verbose) message("Opening: ", target)
return(invisible(.sopen_open(target)))
}
raw <- paste(unlist(args), collapse = " ")
cleaned <- .sopen_strip_quotes(raw)
if (.sopen_is_url(cleaned)) {
if (verbose) message("Opening URL: ", cleaned)
utils::browseURL(cleaned)
return(invisible(cleaned))
}
cleaned <- .sopen_normalize_slashes(cleaned)
candidate <- .sopen_resolve_candidate(cleaned)
open_target <- if (.sopen_exists(candidate)) candidate else cleaned
if (verbose) message("Opening: ", open_target)
invisible(.sopen_open(open_target))
}
.sopen_strip_quotes <- function(x) {
x <- trimws(x)
if (nchar(x) >= 2 && substr(x, 1, 1) %in% c('"', "'") &&
substr(x, nchar(x), nchar(x)) == substr(x, 1, 1)) {
substr(x, 2, nchar(x) - 1)
} else {
x
}
}
.sopen_is_url <- function(x) {
grepl("^(https?://)", x, ignore.case = TRUE)
}
.sopen_exists <- function(path) {
file.exists(path) || dir.exists(path)
}
.sopen_normalize_slashes <- function(path) {
if (.Platform$OS.type == "windows") {
chartr("/", "\\\\", path)
} else {
chartr("\\\\", "/", path)
}
}
.sopen_resolve_candidate <- function(path) {
expanded <- path.expand(path)
if (.sopen_exists(expanded)) {
normalizePath(expanded, winslash = if (.Platform$OS.type == "windows") "\\" else "/", mustWork = FALSE)
} else if (.Platform$OS.type != "windows" &&
grepl("\\.app$", expanded, ignore.case = TRUE)) {
app_path <- file.path("/Applications", basename(expanded))
if (dir.exists(app_path)) return(app_path)
expanded
} else {
expanded
}
}
.sopen_open <- function(target) {
if (.Platform$OS.type == "windows") {
target <- normalizePath(target, winslash = "\\", mustWork = FALSE)
base::shell.exec(target)
} else {
opener <- if (Sys.info()[["sysname"]] == "Darwin") "open" else "xdg-open"
status <- suppressWarnings(system2(opener, shQuote(target), wait = FALSE, stdout = NULL, stderr = NULL))
if (!identical(status, 0L)) stop(sprintf("Failed to open '%s' using %s.", target, opener), call. = FALSE)
}
target
}
sopen("D:/")
sopen("D:/")
sopen("D:/")
sopen("https://www.baidu.com")
sopen()
getwd()
sopen()
sopen()
sopen <- function(..., verbose = TRUE) {
args <- list(...)
if (length(args) == 0) {
target <- getwd()
if (verbose) message("Opening: ", target)
return(invisible(.sopen_open(target)))
}
raw <- paste(unlist(args), collapse = " ")
cleaned <- .sopen_strip_quotes(raw)
if (.sopen_is_url(cleaned)) {
if (verbose) message("Opening URL: ", cleaned)
utils::browseURL(cleaned)
return(invisible(cleaned))
}
cleaned <- .sopen_normalize_slashes(cleaned)
candidate <- .sopen_resolve_candidate(cleaned)
open_target <- if (.sopen_exists(candidate)) candidate else cleaned
if (verbose) message("Opening: ", open_target)
invisible(.sopen_open(open_target))
}
.sopen_strip_quotes <- function(x) {
x <- trimws(x)
if (nchar(x) >= 2 && substr(x, 1, 1) %in% c('"', "'") &&
substr(x, nchar(x), nchar(x)) == substr(x, 1, 1)) {
substr(x, 2, nchar(x) - 1)
} else {
x
}
}
.sopen_is_url <- function(x) {
grepl("^(https?://)", x, ignore.case = TRUE)
}
.sopen_exists <- function(path) {
file.exists(path) || dir.exists(path)
}
.sopen_normalize_slashes <- function(path) {
if (.Platform$OS.type == "windows") {
chartr("/", "\\\\", path)
} else {
chartr("\\\\", "/", path)
}
}
.sopen_resolve_candidate <- function(path) {
expanded <- path.expand(path)
if (.sopen_exists(expanded)) {
normalizePath(expanded, winslash = if (.Platform$OS.type == "windows") "\\" else "/", mustWork = FALSE)
} else if (.Platform$OS.type != "windows" &&
grepl("\\.app$", expanded, ignore.case = TRUE)) {
app_path <- file.path("/Applications", basename(expanded))
if (dir.exists(app_path)) return(app_path)
expanded
} else {
expanded
}
}
.sopen_open <- function(target) {
if (.Platform$OS.type == "windows") {
target <- normalizePath(target, winslash = "\\", mustWork = FALSE)
base::shell.exec(target)
} else {
opener <- if (Sys.info()[["sysname"]] == "Darwin") "open" else "xdg-open"
status <- suppressWarnings(system2(opener, shQuote(target), wait = FALSE, stdout = NULL, stderr = NULL))
if (!identical(status, 0L)) stop(sprintf("Failed to open '%s' using %s.", target, opener), call. = FALSE)
}
target
}
sopen()
sopen
wxsou <- function(kw) {
if (missing(kw) || trimws(kw) == "") {
cat("用法:  wxsou('关键词')\n")
cat("示例:  wxsou('房价+香樟')\n")
return(invisible(NULL))
}
# URL 编码
q <- utils::URLencode(kw, reserved = TRUE)
url <- paste0("https://weixin.sogou.com/weixin?type=2&query=", q)
# 打开浏览器
browseURL(url)
}
# 示例
# wxsou("房价+香樟")
wxsou("房价+香樟")
wxsou("房价 香樟")
wxsou("数学的美")
wxsou("林伯强")
library(devtools); install_github('JeffreyRacine/npiv')
install.packages("devtools")
library(devtools); install_github('JeffreyRacine/npiv')
install.packages('npiv')
install.packages('xtdml')
install.packages('bbotk')
install.packages('bbotk')
install.packages('xtdml')
install.packages('xtdml')
library('xtdml')
library('npiv')
install.packages('npiv')
library('npiv')
?npiv
install.packages("raster")
install.packages("rgdal")
install.packages("stars")
install.packages("exactextractr")
install.packages("ncmeta")
suppressPackageStartupMessages({
library(ncdf4)
library(sf)
library(dplyr)
library(readr)
library(terra)
})
nc <- ncdf4::nc_open(nc_path)
setwd("D:/xwechat_files/wxid_vu6eu977ndi121_3458/msg/migrate/File/2025-03/zonalstatistic_data/")
nc <- ncdf4::nc_open(nc_path)
nc <- ncdf4::nc_open("hunan.nc")
lon <- ncdf4::ncvar_get(nc, "lon")
lat <- ncdf4::ncvar_get(nc, "lat")
time <- ncdf4::ncvar_get(nc, "time")
time_units <- ncdf4::ncatt_get(nc, "time", "units")
ncdf4::nc_close(nc)
nc <- ncdf4::nc_open("hunan.nc")
lon <- ncdf4::ncvar_get(nc, "lon")
lat <- ncdf4::ncvar_get(nc, "lat")
time <- ncdf4::ncvar_get(nc, "time")
time_units <- ncdf4::ncatt_get(nc, "time", "units")
var_name <- "tas"
var <- nc$var[[var_name]]
arr <- ncdf4::ncvar_get(nc, var_name)
dim_names <- sapply(var$dim, function(x) x$name)
ncdf4::nc_close(nc)
var$dim
var$dim$name
var$dim[1]
var$dim[[1]]
var$dim[1]
var$dim[2]
var$dim[3]
setwd("D:/xwechat_files/wxid_vu6eu977ndi121_3458/msg/migrate/File/2025-03/zonalstatistic_data/")
nc <- ncdf4::nc_open("hunan.nc")
lon <- ncdf4::ncvar_get(nc, "lon")
lat <- ncdf4::ncvar_get(nc, "lat")
time <- ncdf4::ncvar_get(nc, "time")
time_units <- ncdf4::ncatt_get(nc, "time", "units")
var_name <- "tas"
var <- nc$var[[var_name]]
arr <- ncdf4::ncvar_get(nc, var_name)
dim_names <- sapply(var$dim, function(x) x$name)
arr <-aperm(arr, c("lon", "lat", "time"))
source('D:/xwechat_files/wxid_vu6eu977ndi121_3458/msg/migrate/File/2025-03/zonalstatistic_data/nc_zonal_hunan.r')
res <- nc_zonal_hunan(D:/xwechat_files/wxid_vu6eu977ndi121_3458/msg/migrate/File/2025-03/zonalstatistic_data/hunan.nc,'D:/xwechat_files/wxid_vu6eu977ndi121_3458/msg/migrate/File/2025-03/zonalstatistic_data/hunan.shp', out_csv = NULL, time_mode='avg')
res <- nc_zonal_hunan("D:/xwechat_files/wxid_vu6eu977ndi121_3458/msg/migrate/File/2025-03/zonalstatistic_data/hunan.nc",'D:/xwechat_files/wxid_vu6eu977ndi121_3458/msg/migrate/File/2025-03/zonalstatistic_data/hunan.shp', out_csv = NULL, time_mode='avg')
